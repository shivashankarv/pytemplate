{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c21d0c-4d60-4eb9-a9ba-5a6c1ea82eea",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "* Make a bigram count and probability matrix.\n",
    "* Train a bigram language model in `pytorch` that takes an input bigram and predicts the next character.\n",
    "   * Make sure the counts matrix is similar to the count matrix above.\n",
    "* Get new names by drawing from the probability distribution of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fedda6a-8e20-48b2-bda8-cb09f546071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca89e1c-a426-4716-81b8-90029c8f87be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read `words.txt` as a list\n",
    "words = open(\"../names.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a819688-c947-46d7-813f-3a7b1efe9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the bigram counts matrix\n",
    "v = 27 # vocab_size\n",
    "N = torch.zeros((v, v), dtype=torch.int32) # count matrix\n",
    "\n",
    "tokens = [\".\"] + sorted(set(list(\"\".join(words))))\n",
    "stoi = dict([(c,i) for i,c in enumerate(tokens)])\n",
    "itos = dict([(i,c) for c,i in stoi.items()])\n",
    "\n",
    "# Get bigrams\n",
    "for w in words:\n",
    "    chars = list(\".\" + w + \".\")\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb81d90-2275-4371-a4e7-0588564da2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13268a7-0527-4315-8926-27bee5f9986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the probability matrix\n",
    "# Adding smoothing to avoid zero division error.\n",
    "P = (N + 1).float()\n",
    "P /= P.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e003e5-b7b6-4225-93c9-361a3fce7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Way to sample from distribution.\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "torch.multinomial(torch.rand(3, generator=g), 100, replacement=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d73cc-0d1a-4d48-8c27-c2e824326da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating new words by drawing from the distribution using the same strategy as above.\n",
    "# We need to start with .\n",
    "# Randomly draw an index from P[0] and keep going from there.\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix = 0\n",
    "num_words = 5\n",
    "\n",
    "for word in range(num_words):\n",
    "    word = \"\"\n",
    "    while True:\n",
    "        ix = torch.multinomial(P[ix], 1, replacement=True, generator=g).item()\n",
    "        word += itos[ix]\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565bb267-c6a4-4f45-93ea-84e8cce266ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLL\n",
    "# Goal is to sum up the likelihood of a name and transform it as a loss\n",
    "# Adding loss term to the names above.\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "ix = 0\n",
    "num_words = 5\n",
    "\n",
    "for word in range(num_words):\n",
    "    word = \"\"\n",
    "    nll = 0.0\n",
    "    while True:\n",
    "        prev_ix = ix\n",
    "        ix = torch.multinomial(P[ix], 1, replacement=True, generator=g).item()\n",
    "        word += itos[ix]\n",
    "        nll += -P[prev_ix, ix].log()\n",
    "        if ix == 0:\n",
    "            break\n",
    "    l = len(word)\n",
    "    print(word, (nll/l).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a1823-501f-41f8-9b32-8024a44af015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coming up with a scoring system to evaluate the word-likeness of new words.\n",
    "#-sum(log(p))\n",
    "## Result should be:\n",
    "#\n",
    "#log_likelihood=tensor(-559951.5625)\n",
    "#nll=tensor(559951.5625)\n",
    "#2.4543561935424805\n",
    "log_likelihood = 0\n",
    "n = 0\n",
    "# Calculating the sum of likelihoods in names in `names.txt`\n",
    "for w in words:\n",
    "    chars = list(\".\" + w + \".\")\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cdc950-8858-4baa-8f59-59dc8e34faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the Neural Network\n",
    "# Strategy:\n",
    "# Input token -> X\n",
    "# Output token -> Y\n",
    "# Onehot encode the bigrams -> Xenc.\n",
    "# Initialize W\n",
    "# W @ Xenc -> logits\n",
    "# logits.exp() -> counts\n",
    "# counts / counts.sum(1, keepdim=True) -> P\n",
    "# -P.log().sum() -> loss\n",
    "\n",
    "# Compile X into a tensor\n",
    "# Get bigrams\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chars = list(\".\" + w + \".\")\n",
    "    for ch1, ch2 in zip(chars, chars[1:]):\n",
    "        xs += [stoi[ch1]]\n",
    "        ys += [stoi[ch2]]\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "# Weights\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.rand((v, v), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f337ffd-8261-47ba-981d-108ed71c949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 70.0\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    # Onehot encoding of input\n",
    "    xenc = F.one_hot(xs, num_classes=v).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    # Summing up the probabilities at the yth position alone.\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(f\"{epoch=} | {loss.item()=}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    W.data += -step * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284e509-e77e-4d81-8d0d-0c343fadf910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sampling names from neural network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "num_word = 5\n",
    "new_words = []\n",
    "\n",
    "for i in range(num_word):\n",
    "    # `name` will contain the generated name\n",
    "    word = []\n",
    "    ix = 0 #this is the index for the .* bigrams, that start a name.\n",
    "    \n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "        #Randomly draw a second letter, or a column\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item()\n",
    "        word.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            new_words.append(\"\".join(word))\n",
    "            break\n",
    "print(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbff6c8-d553-4a01-abfc-d156162003d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toynn",
   "language": "python",
   "name": "toynn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
