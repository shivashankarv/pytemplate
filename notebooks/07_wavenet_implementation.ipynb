{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ad2c8-abd0-4907-adad-f2a3069f75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4217f59-5dfc-49e0-a4b0-0c0e609146c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"../names.txt\").read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = dict([(c, i) for i, c in enumerate(['.']+chars)])\n",
    "itos = dict([(i, c) for c, i in stoi.items()])\n",
    "vocab_size = len(stoi.keys())\n",
    "block_size = 8 # Context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e3296-a244-498e-b1c6-87ce80e6a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making X and Y\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * block_size # Starting with 3 dots. Default that leads to the first Y token prediction.\n",
    "        \n",
    "        for ch in w + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            #print(\"\".join([itos[i] for i in context]) + \" --> \" + ch)\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "random.seed = 42\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "# Train, dev, test\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xtest, Ytest = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb7909-0d4e-4c6d-b246-5acaa4536e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f1f9e-b9e5-48b4-b5c6-5f9b83ae8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement the pytorch layers to jog our memory.\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=False):\n",
    "        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        self.bias = torch.randn(fan_out) if bias is True else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, momentum=0.1, eps=1e-5):\n",
    "        self.training = True\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            # fixing the batch norm bug for the new unit batch for wavenet\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0, 1)\n",
    "            xmean = x.mean(dim, keepdims=True)\n",
    "            xvar = x.var(dim, keepdims=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        if self.training is True:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = self.running_mean * (1 - self.momentum) + self.momentum * xmean\n",
    "                self.running_var = self.running_var * (1 - self.momentum) + self.momentum * xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, num_embeddings, embed_dim):\n",
    "        self.weight = torch.randn((num_embeddings, embed_dim))\n",
    "\n",
    "    def __call__(self, ix):\n",
    "        self.out = self.weight[ix]\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.view(x.shape[0], -1)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "# Implementing Wavenet\n",
    "class FlattenConsecutive:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        x = x.view(B, T//self.n, C*self.n)\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.squeeze(1)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for l in self.layers for p in l.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276ea14-25c8-4d92-b0e6-e767a12d06fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd2e07-0a3b-4854-9fac-67f165d53b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = torch.randint(0, Xtr.shape[0], (4,))\n",
    "Xs, Ys = Xtr[ix], Ytr[ix]\n",
    "logits = model(Xs)\n",
    "loss = F.cross_entropy(logits, Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2842cc-afb8-4b2d-972f-da4f2ef9a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.__class__.__name__, layer.out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e276a3-c49d-478c-a05c-55163912af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-7].running_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a3f9e-2b35-4c93-a30c-821df8fe3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb = 24\n",
    "#n_hidden = 200\n",
    "#n_hidden = 68\n",
    "n_hidden = 128\n",
    "unit = 2\n",
    "\n",
    "# Wavenet style\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, n_emb),\n",
    "    FlattenConsecutive(unit), Linear(n_emb * unit, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(unit), Linear(n_hidden * unit, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n",
    "    FlattenConsecutive(unit), Linear(n_hidden * unit, n_hidden), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size),\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "    # last layer: make less confident\n",
    "    #layers[-1].gamma *= 0.1\n",
    "    model.layers[-1].weight *= 0.1\n",
    "    # all other layers: apply gain\n",
    "    for layer in model.layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 5/3\n",
    "\n",
    "parameters = [p for l in model.layers for p in l.parameters()]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "stepi = []\n",
    "lossi = []\n",
    "ud = []\n",
    "batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31a8b1-de46-43dd-b85a-07b26c29ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200000\n",
    "lr = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    logits = model(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    for layer in model.layers:\n",
    "        layer.out.retain_grad()\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    lr = 0.1 if epoch < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    stepi.append(epoch)\n",
    "    lossi.append(loss.log10().item())\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f'{epoch:7d}/{epochs:7d}: {loss.item():.4f}')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "    # if epoch == 1000:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab6bebd-c6ca-4df3-a746-ccb80346b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7ad23-6f5b-44d1-bced-8453c982a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xtest, Ytest),\n",
    "    }[split]\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "# put layers into eval mode\n",
    "for layer in model.layers:\n",
    "  layer.training = False\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9905362c-b0e2-4710-90c4-cd8345d209c4",
   "metadata": {},
   "source": [
    "### MLP + BatchNorm1d\n",
    "_train_: 1.7142295837402344\n",
    "_val_: 1.9895931482315063\n",
    "### WaveNet - emb: 10, nhidden: 64\n",
    "_train_: 1.9279804229736328\n",
    "_val_: 2.041594982147217\n",
    "### WaveNet - emb: 24, nhidden: 128\n",
    "_train_: 1.793333649635315\n",
    "_val_: 2.0019969940185547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea5cb84-120a-4b74-ba4e-b0a1acb5ea75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:toynn]",
   "language": "python",
   "name": "conda-env-toynn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
