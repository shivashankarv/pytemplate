{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfd892d-98a7-45b9-a1d8-0dbd7b10e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db8252f-9c0d-4f7e-ab7e-c84668346f6c",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56012f68-dad1-4a15-bd75-d5367a852208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3eb81-db8c-40cf-8fcb-ed4799ae10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c729d2c-edbc-428a-9fb6-01fd0e3e667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511f484-fcc5-4524-968a-19d3a9c34996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7adf79-0031-4cca-982d-364f97a88a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the vocab size\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars), vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651b68a8-0079-43a9-9d80-791522bedff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda i: \"\".join([itos[t] for t in i])\n",
    "\n",
    "print(encode(\"hii there.\"))\n",
    "print(decode(encode(\"hii there.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d51cc-3c0c-4370-8a36-7607769fb2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the entire tiny shakespeare\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c37cf-3f40-4cf2-95c5-ffa5fd583106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5975e87-62fb-4323-baa5-739ba2dd47eb",
   "metadata": {},
   "source": [
    "# A word on training\n",
    "While training these transformers we only train them on sample chunks of the text at a time, in parallel, at a fixed length. This fixed length is what we term as `block_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7035d-6418-4a3a-935d-2d104b560df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are defining our block_size here - context length\n",
    "block_size = 8\n",
    "\n",
    "# This is going to be one training example.\n",
    "# We are going to generate multiple training examples similar to the way we, given a name in our previous models.\n",
    "example = train_data[:block_size+1]\n",
    "print(f\"{example=}\")\n",
    "x_example = []\n",
    "for i in range(block_size):\n",
    "    x_example.append(example[i].item())\n",
    "    print(f\"Given {x_example} predict --> {example[i+1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5706e0a-7329-4e20-838c-3fd4c7e89613",
   "metadata": {},
   "source": [
    "# Retrieve sample chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9249ff5-8ac2-4ddf-a22e-5ca4afe4e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split=\"train\"):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    # Gets for random start_ix\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch()\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c8405d-df0a-4622-9c86-0a8189aed899",
   "metadata": {},
   "source": [
    "# Quick recap:\n",
    "* `Bigram` model captured the co-occurence frequencies of 2 tokens.\n",
    "* The Neural Network approach was to start a random frequency matrix and forward and backprop until we got this.\n",
    "* We then found that this wasn't so good and decided to use an embedding matrix instead that condensed frequency information with bigger context.\n",
    "* Finally we used the wavenet idea and decomposed large contexts as composits of bigrams that propogated upwards.\n",
    "This is where we are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880aa45-c4bf-401e-acb0-ffcf86e9c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimplementing bigramNN in pytorch proper\n",
    "#n_embd = 10\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize the word-frequency or weight matrix\n",
    "        #self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        # Rewriting this to incorporate word embeddings C instead\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx = index of Xb, targets = Yb\n",
    "        # This is equivalent to one-hot(xenc) @ W, which is a lookup table.\n",
    "        logits = self.token_embedding_table(idx) # B, T, C\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # We need to reshape this to (B, C, T) as thats what pytorch uses.\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B, T) and you want to take this and extend to (B, T + max_new_tokens)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get logits and loss is None because targets are None.\n",
    "            logits, loss = self(idx)\n",
    "            # Get the logits at the Tth time dimension to predict T+1.\n",
    "            logits = logits[:, -1, :]\n",
    "            # Get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample from probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # B, 1\n",
    "            # Shifting to the T+1th token.\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # B, T+1\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7ae0c-8021-4316-94b6-d929feaa5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BigramLanguageModel()\n",
    "out, loss = m(xb, yb)\n",
    "print(out.shape, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced989b-b28f-4094-9d75-e0d0be469d1e",
   "metadata": {},
   "source": [
    "### Generating some output with the untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2a800-9998-4db5-9355-d3e756b0a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_ix = torch.zeros((1,1), dtype=torch.long)\n",
    "print(\"\\n--\\n\".join([decode(i) for i in m.generate(seed_ix, 1000).tolist()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5897a3-3a1c-4809-ad67-231d18b7e136",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794879b-477d-4b61-997b-d6c445db42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting an optimizer object\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81ccb8-c16f-4080-952c-85a7ff87e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for _ in range(10000):\n",
    "    # Get sample data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    # Set grad to zero\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236eb7fa-a618-4664-b36f-82c38a60488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_ix = torch.zeros((1,1), dtype=torch.long)\n",
    "print(\"\\n--\\n\".join([decode(i) for i in m.generate(seed_ix, 300).tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d02846-16ac-4564-a6ab-8d4f7e4294eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:toynn]",
   "language": "python",
   "name": "conda-env-toynn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
