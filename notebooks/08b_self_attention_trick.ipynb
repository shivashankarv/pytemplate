{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f82087-ab1f-46f4-8d51-c7ab9ab31e38",
   "metadata": {},
   "source": [
    "# Math trick to attention\n",
    "In the previous bigram model we aren't getting the tokens to speak to each other to find affinities. The current token being processed, can only take the preceding tokens as context to calculate attention on.\n",
    "\n",
    "We simply apply an upper triangle mask to block context from the tokens of the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a856c023-7529-4dec-8310-9cb9e6d12b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:toynn]",
   "language": "python",
   "name": "conda-env-toynn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
