{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e235a7f-205a-421e-a3c1-60ba8cba64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247fb759-4210-466e-957b-c14b434226ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"../names.txt\").read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = dict([(c, i) for i, c in enumerate(['.']+chars)])\n",
    "itos = dict([(i, c) for c, i in stoi.items()])\n",
    "vocab_size = len(stoi.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfaaba-5670-4ae2-a0b9-9b54db840964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep\n",
    "block_size = 3\n",
    "def build_dataset(words, block_size=3):\n",
    "    # Constructing the input matrix\n",
    "    X, Y = [],[]\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            #print(\"\".join([itos[i] for i in context]) + \" ---> \" + ch)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xtest, Ytest = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ddf31-1cf0-4acf-ac28-4fa5418d6919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "emb_dim = 10\n",
    "hidden_dim = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, emb_dim), generator=g)\n",
    "# Change 2 - Reducing the range of W1 results in a reduction in the number of dead neurons.\n",
    "#W1 = torch.randn((block_size * emb_dim, hidden_dim), generator=g) * 0.2\n",
    "# Change 5 - Adding the bnbias term renders the `b1` matrix unnecessary and spurious. \n",
    "#b1 = torch.randn(hidden_dim, generator=g) * 0.01\n",
    "# Change 3 - Using a gain function takes guesswork out of this.\n",
    "W1 = torch.randn((block_size * emb_dim, hidden_dim), generator=g) * ((5/3) / (block_size * emb_dim)**0.5)\n",
    "#b1 = torch.randn(hidden_dim, generator=g) * 0.01\n",
    "\n",
    "# Change 1 - Reducing the range of initial weights and setting bias to zero helps with the initialization problem.\n",
    "#W2 = torch.randn((hidden_dim, vocab_size), generator=g)\n",
    "#b2 = torch.randn(vocab_size, generator=g)\n",
    "W2 = torch.randn((hidden_dim, vocab_size), generator=g) * 0.01 # target is E[W2] = 1/27; 40 becomes 4\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0\n",
    "\n",
    "# Change 4 - BatchNorm paper - mean normalizing the weights(gain or gamma) and bias(or beta) forces gaussian distribution.\n",
    "bngain = torch.ones((1, hidden_dim))\n",
    "bnbias = torch.zeros((1, hidden_dim))\n",
    "\n",
    "# Change 5 - Setting a running mean and std makes predicting values easy. No recalculating the mean and std.\n",
    "bnmean_running = torch.zeros((1, hidden_dim))\n",
    "bnstd_running = torch.ones((1, hidden_dim))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "lossi = []\n",
    "stepi = []\n",
    "\n",
    "minibatch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca888c4-0ad8-44ca-9546-da82b7631315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the network\n",
    "epochs = 200000\n",
    "#lr = 0.01\n",
    "for epoch in range(epochs):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (minibatch_size,))\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    # Change 5\n",
    "    hpreact = embcat @ W1 #+ b1\n",
    "    bnmeani = hpreact.mean(0, keepdim=True)\n",
    "    bnstdi = hpreact.std(0, keepdim=True)\n",
    "    hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "        bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    #lr = lrs[epoch]\n",
    "    lr = 0.1 if epoch < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    stepi.append(epoch)\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f'{epoch:7d}/{epochs:7d}: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58783099-58b9-45a3-9c6b-e8b543af89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xtest, Ytest),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 # + b1\n",
    "  hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf7180-e0f5-4368-9405-a601ce6fa47e",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22be88-0099-4553-af88-d21b9f251ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepi, lossi);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb68c77c-7f9c-40f5-9117-8b98ba52f4c3",
   "metadata": {},
   "source": [
    "We want to get rid of the hockey stick like optimization of loss at the beginning.\n",
    "More specifically, we want the `0/ 200000: 23.9692` loss at the first 10000 steps to be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551f4cd6-6090-4219-bda1-c876479b9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the logits after the first epoch?\n",
    "plt.hist(logits.view(-1).tolist());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e34b4e4-c644-4939-8dee-f234ef870543",
   "metadata": {},
   "source": [
    "Takes on very extreme values. We would expect this to be drawn from a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ccc38-bc67-4bfd-9eed-932996005220",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((h.abs() > 0.9999).tolist(), cmap=\"Grays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886d37ac-4a96-4402-a475-d36f0f47037e",
   "metadata": {},
   "source": [
    "The white blocks there show the neurons that aren't firing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d95f058-4471-4f30-a2ad-93b7fdb721e2",
   "metadata": {},
   "source": [
    "# Change 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bac1a6-11db-49c1-b070-1880db69e2bc",
   "metadata": {},
   "source": [
    "# What can we do?\n",
    "We want to see to it that the logits are drawn from a uniform distribution.\n",
    "probs -> counts -> logits -> W2, b2, h -> W1, b1\n",
    "\n",
    "So scaling down W2 and b2 for the first epoch should help scale this down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d289766-1056-4083-b4d1-bf42367fe9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((W2 * 0.01).view(-1).tolist());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8b139-3775-4ad5-bbd8-3affa44ad868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what a uniform distribution looks like.\n",
    "1.0/27.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b40764-78e2-4bde-823f-548bcf02fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 1\n",
    "plt.plot(stepi, lossi);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6dc84a-ba0c-4280-bda2-4d4ba38f1bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((W2).view(-1).tolist());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb6d5bf-d6ae-4806-b091-55db8156346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((h.abs() > 0.9999).tolist(), cmap=\"Grays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf3d84-da47-40f9-81bd-ec9c61f1fa84",
   "metadata": {},
   "source": [
    "We still have the `h` problem and hence inactive neurons `tanh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454977e1-b570-433e-9b00-e82a1dd1a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(h.view(-1).tolist());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7665caa-3444-4c10-9540-8797de294995",
   "metadata": {},
   "source": [
    "We have a high number of 1's and -1's that the `tanh` function squashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5823dba-9f92-44ba-9fcd-7406bcf934fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hpreact.view(-1).tolist());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b02440-9c84-4b32-a2db-04faf8778f83",
   "metadata": {},
   "source": [
    "# Change 2\n",
    "`hpreact` takes on a wide range of values causing many neurons to be inactivated.\n",
    "\n",
    "hpreact -> W1, b1\n",
    "\n",
    "So scaling down W1 and b1 should help.\n",
    "We want it to be unit gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e24fcae-0a0a-425e-9cf0-2405e5badb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepi, lossi);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc1a51-ccce-4136-b635-7da556c25ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(h.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4424e4-23aa-4324-b935-16ab43aa95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((h.abs() > 0.99).tolist(), cmap=\"gray\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd0d56-b51e-4099-901e-08ff274d1b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hpreact.view(-1).tolist());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc73751a-bffb-4341-9346-955de6474374",
   "metadata": {},
   "source": [
    "# Change 3\n",
    "Is there a better way to do this?\n",
    "\n",
    "Kaiming He, scale by the `gain/sqrt(fan_in)` where `gain` is predetermined for each non-linearity\n",
    "tanh = 5/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc0246-72d3-4574-a473-1d5c73b5f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepi, lossi);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aad839-5ffd-4315-8797-cec013b2a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(h.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32a89a-a01b-4afa-b6b4-9404db4ca845",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((h.abs() > 0.99).tolist(), cmap=\"gray\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7925ab-683d-446f-96eb-d3d000f6d756",
   "metadata": {},
   "source": [
    "# Change 4\n",
    "\n",
    "`BatchNorm` - Batch normalization\n",
    "\n",
    "Mean normalize the the entire `hpreact` to ensure unit normal gaussian distribution.\n",
    "We then multiply this term by a gain and add a bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02bb0a-6f7a-48b0-8103-9291dbbf09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepi, lossi);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255ebe4-a355-4a10-86ab-dcc3008e6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((h.abs() > 0.99).tolist(), cmap=\"gray\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc6a49-56f1-4932-aaf0-12901a347f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toynn",
   "language": "python",
   "name": "toynn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
